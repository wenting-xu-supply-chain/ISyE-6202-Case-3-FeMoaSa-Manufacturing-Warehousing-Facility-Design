# -*- coding: utf-8 -*-
"""
ISyE 6202 & 6335 - Casework 3
Task 4: Multi-Year Capacity Plan (Years +2..+5)

Outputs per year (+2..+5), all suffixed with YpN and placed under Task4_Outcome_All_Demands/Year+N:
- Task4_Full_Capacity_Annual_Weekly_Robust_YpN.csv
- Task4_Per_Part_Weekly_Mean_Std_Robust_YpN.csv
- Task4_Per_Part_Annual_Mean_Std_Robust_YpN.csv
- Product_Demand_Annual_Avg_YpN.csv
- Product_Demand_Annual_StdDev_YpN.csv
- Product_Demand_Weekly_Avg_YpN.csv
- Product_Demand_Weekly_StdDev_YpN.csv
- Parts_Per_Product_BOM_YpN.csv
- Parts_Total_Demand_By_Family_AnnualAvg_YpN.csv
- Parts_Total_Demand_By_Family_WeeklyAvg_YpN.csv
- Parts_By_Family_Weekly_Robust_Units_and_GrossMinutes_YpN.csv     (NEW: allocation consistent with FULL)
- Task4_Audit_Reconciliation_YpN.csv                                (NEW: row-sum vs FULL)
- Task4_Audit_Summary_YpN.txt                                       (NEW: max abs diffs)
- Task4_Audit_FamilyColumnSums_YpN.csv                              (NEW: allocation vs naive family-first)
"""

import os, re, math
from pathlib import Path
from typing import Dict, List, Iterable
import numpy as np
import pandas as pd
from scipy.stats import norm

# ---------------- Tunables ----------------
EFFICIENCY      = 0.90
RELIABILITY     = 0.98
SERVICE_LEVEL   = 0.995                         # one-sided 99.5%
Z               = norm.ppf(SERVICE_LEVEL)       # ≈ 2.575829
WEEKS_PER_YEAR  = 52.0
YEARS           = [2, 3, 4, 5]

# >>> Update if your workbook filename differs <<<
EXCEL_PATH      = "iSYE 6202 & 6335 2025 Casework 3 FaMoaSa Facility Design - Tables and Basic Layouts.xlsx"
ROOT_OUT        = "Task4_Outcome_All_Demands"

FAM_RE          = re.compile(r"^[AB]\d+$")
YEAR_TAGS       = {f"+{y}" for y in YEARS}

# ---------------- Utilities ----------------
def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

def natural_sort(labels: Iterable[str]) -> List[str]:
    """Natural order like P1, P2, ..., P10."""
    def key(s):
        s = str(s)
        num = "".join(ch for ch in s if ch.isdigit())
        return (s.rstrip(num), int(num) if num else math.inf)
    return sorted(labels, key=key)

def to_float(x) -> float:
    """Robust numeric parsing for plain numbers (commas, parentheses, etc.)."""
    if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):
        return float(x)
    if x is None:
        return np.nan
    s = str(x).strip()
    if s == "" or s.lower() in {"na","nan","none","-"}:
        return np.nan
    neg = False
    if s.startswith("(") and s.endswith(")"):
        neg = True
        s = s[1:-1]
    s = s.replace(",", "").strip()
    m = re.match(r"^[+-]?\d+(?:\.\d+)?", s)
    if not m:
        return np.nan
    v = float(m.group(0))
    return -v if neg else v

def to_float_pct(x) -> float:
    """Percent-aware numeric parsing: '12.5%'->0.125, '(15%)'->-0.15, '0.2' stays 0.2."""
    if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):
        return float(x)
    if x is None:
        return np.nan
    s = str(x).strip()
    if s == "" or s.lower() in {"na","nan","none","-"}:
        return np.nan
    neg = False
    if s.startswith("(") and s.endswith(")"):
        neg = True
        s = s[1:-1]
    s = s.replace(",", "").strip()
    if "%" in s:
        s = s.replace("%", "").strip()
        m = re.match(r"^[+-]?\d+(?:\.\d+)?", s)
        if not m:
            return np.nan
        v = float(m.group(0)) / 100.0
        return -v if neg else v
    m = re.match(r"^[+-]?\d+(?:\.\d+)?", s)
    if not m:
        return np.nan
    v = float(m.group(0))
    return -v if neg else v

def row_labels_map(row_vals) -> Dict[str, int]:
    """Map exact string cell -> column index for a given row."""
    out = {}
    for c, v in enumerate(row_vals):
        if isinstance(v, str):
            out[v.strip()] = c
    return out

def detect_families_in_row(raw: pd.DataFrame, r: int) -> List[str]:
    """Detect all A*/B* family labels in a specific row, preserving left-to-right order."""
    fams = []
    for c in range(raw.shape[1]):
        v = raw.iat[r, c]
        if isinstance(v, str) and FAM_RE.fullmatch(v.strip()):
            fams.append(v.strip())
    return fams

def collect_year_rows(raw: pd.DataFrame, header_row: int, year_col: int,
                      fam_cols: Dict[str,int], fam_order: List[str],
                      value_parser=to_float, scan_down=120) -> Dict[str, List[pd.Series]]:
    """Collect ALL candidate rows for +2..+5 under a given header row."""
    nrows = raw.shape[0]
    out: Dict[str, List[pd.Series]] = {f"+{y}": [] for y in YEARS}
    for r in range(header_row+1, min(header_row+1+scan_down, nrows)):
        yv = raw.iat[r, year_col]
        if isinstance(yv, str) and yv.strip() in YEAR_TAGS:
            tag = yv.strip()
            vals = [value_parser(raw.iat[r, fam_cols[f]]) if f in fam_cols else np.nan for f in fam_order]
            out[tag].append(pd.Series(vals, index=fam_order, dtype="float64").fillna(0.0))
    return out

def choose_by_sum(cands: List[pd.Series]) -> pd.Series:
    """Choose the candidate with the largest total; return zeros if none."""
    if not cands:
        return pd.Series(dtype="float64")
    sums = [float(s.sum()) for s in cands]
    idx = int(np.nanargmax(sums))
    return cands[idx].astype(float).fillna(0.0)

# ---------------- Read +2..+5 demand (mean & std or cv) ----------------
def read_demand_mean_std_or_cv(xl_path: str):
    xls = pd.ExcelFile(xl_path)
    sheet = None
    for cand in ["+2 to +5 Year Product Demand ", "+2 to +5 Year Product Demand"]:
        if cand in xls.sheet_names:
            sheet = cand
            break
    if sheet is None:
        raise ValueError("Sheet '+2 to +5 Year Product Demand' not found.")
    raw = pd.read_excel(xl_path, sheet_name=sheet, header=None)

    # Mean headers
    mean_headers = []
    for r in range(min(200, raw.shape[0])):
        labels = row_labels_map(raw.iloc[r].tolist())
        if "Year" in labels:
            fams = detect_families_in_row(raw, r)
            if len(fams) >= 3:
                fam_order = fams[:]
                fam_cols  = {f: labels[f] for f in fam_order if f in labels}
                mean_headers.append((r, fam_order, fam_cols, labels["Year"]))
    if not mean_headers:
        raise ValueError("Mean header row not found (need 'Year' and multiple family labels).")

    # Unified family order
    family_seen = []
    for r, fam_order, _, _ in mean_headers:
        for f in fam_order:
            if f not in family_seen:
                family_seen.append(f)
    fam_order_unified = family_seen

    # Mean candidates
    mean_cands: Dict[str, List[pd.Series]] = {f"+{y}": [] for y in YEARS}
    for r, fam_order, fam_cols, ycol in mean_headers:
        fam_cols_expanded = {f: fam_cols.get(f, None) for f in fam_order_unified}
        local_order = [f for f in fam_order_unified if fam_cols_expanded.get(f) is not None]
        local_cols  = {f: fam_cols_expanded[f] for f in local_order}
        cands = collect_year_rows(raw, r, ycol, local_cols, local_order, value_parser=to_float, scan_down=120)
        for tag, ser_list in cands.items():
            for s in ser_list:
                mean_cands[tag].append(s.reindex(fam_order_unified).fillna(0.0))

    mean_by_year = {}
    for y in YEARS:
        tag = f"+{y}"
        chosen = choose_by_sum(mean_cands[tag])
        if chosen.empty:
            raise ValueError(f"No mean candidates found for {tag}.")
        mean_by_year[tag] = chosen

    # Std or CV blocks
    nrows, ncols = raw.shape
    std_titles = []
    for r in range(min(nrows, 400)):
        txt = " ".join(str(x) for x in raw.iloc[r, :min(20, ncols)].tolist() if pd.notna(x))
        if re.search(r"Standard\s+Deviation", txt, flags=re.IGNORECASE):
            std_titles.append(r)

    std_cands: Dict[str, List[pd.Series]] = {f"+{y}": [] for y in YEARS}
    used_cv = False

    for title_row in std_titles:
        for r in range(title_row+1, min(title_row+40, nrows)):
            labels = row_labels_map(raw.iloc[r].tolist())
            if "Year" not in labels:
                continue
            fams = detect_families_in_row(raw, r)
            if len(fams) < 3:
                continue
            fam_cols = {f: labels[f] for f in fams if f in labels}
            ycol     = labels["Year"]
            cands = collect_year_rows(raw, r, ycol, fam_cols, fams, value_parser=to_float, scan_down=120)
            for tag, ser_list in cands.items():
                for s in ser_list:
                    std_cands[tag].append(s.reindex(fam_order_unified).fillna(0.0))

    # If no Std, try CV -> Std
    if all(len(v) == 0 for v in std_cands.values()):
        cv_titles = []
        for r in range(min(nrows, 400)):
            txt = " ".join(str(x) for x in raw.iloc[r, :min(20, ncols)].tolist() if pd.notna(x))
            if re.search(r"(Coefficient\s+of\s+Variation|CV\b)", txt, flags=re.IGNORECASE):
                cv_titles.append(r)
        for title_row in cv_titles:
            for r in range(title_row+1, min(title_row+40, nrows)):
                labels = row_labels_map(raw.iloc[r].tolist())
                if "Year" not in labels:
                    continue
                fams = detect_families_in_row(raw, r)
                if len(fams) < 3:
                    continue
                fam_cols = {f: labels[f] for f in fams if f in labels}
                ycol     = labels["Year"]
                cands = collect_year_rows(raw, r, ycol, fam_cols, fams, value_parser=to_float_pct, scan_down=120)
                for tag, ser_list in cands.items():
                    mean_s = mean_by_year.get(tag)
                    if mean_s is None:
                        continue
                    for cv_s in ser_list:
                        cv_u = cv_s.reindex(fam_order_unified).fillna(0.0)
                        std_cands[tag].append((cv_u * mean_s).astype(float).fillna(0.0))
        used_cv = True

    std_by_year = {}
    for y in YEARS:
        tag = f"+{y}"
        chosen_std = choose_by_sum(std_cands[tag])
        if chosen_std.empty:
            chosen_std = pd.Series([0.0]*len(fam_order_unified), index=fam_order_unified, dtype="float64")
        std_by_year[tag] = chosen_std

    # Quick diagnostics
    for y in YEARS:
        tag = f"+{y}"
        mean_s = mean_by_year[tag]
        std_s  = std_by_year[tag]
        with np.errstate(divide='ignore', invalid='ignore'):
            ratio = (std_s / mean_s.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan)
        med_ratio = float(np.nanmedian(ratio.values))
        print(f"[DIAG] {tag}: sum(mean)={float(mean_s.sum()):.0f} | median(std/mean)={med_ratio:.4f} "
              f"{'(from CV)' if used_cv else '(from Std/0)'}")

    return fam_order_unified, mean_by_year, std_by_year

# ---------------- BOM reading (robust) ----------------
def read_bom_consolidated(xl_path: str, fam_order: List[str]) -> pd.DataFrame:
    xls = pd.ExcelFile(xl_path)
    sheet = None
    for cand in ["+2 to +5 Year Parts per Product", "+2 to +5 Year Parts per Product "]:
        if cand in xls.sheet_names:
            sheet = cand
            break
    if sheet is None:
        raise ValueError("Sheet '+2 to +5 Year Parts per Product' not found.")
    raw = pd.read_excel(xl_path, sheet_name=sheet, header=None)
    nrows, ncols = raw.shape

    # Detect 'Part' column via Pxx frequency
    part_col, best_hits = None, -1
    for c in range(ncols):
        hits = raw.iloc[:min(600, nrows), c].astype(str).str.fullmatch(r"P\d+").sum()
        if hits > best_hits:
            best_hits, part_col = hits, c
    if part_col is None or best_hits < 5:
        raise ValueError("BOM: cannot detect 'Part' column by Pxx frequency.")

    # Detect family columns in the header region
    fam_to_col = {}
    for f in fam_order:
        found = False
        for r in range(min(80, nrows)):
            for c in range(ncols):
                v = raw.iat[r, c]
                if isinstance(v, str) and v.strip() == f:
                    fam_to_col[f] = c
                    found = True
                    break
            if found:
                break
    missing = [f for f in fam_order if f not in fam_to_col]
    if missing:
        print(f"[WARN] BOM missing family columns: {missing}. Treating as zeros.")

    # Read body
    parts = []
    data = {f: [] for f in fam_order}
    streak = 0
    for r in range(nrows):
        p = raw.iat[r, part_col]
        if isinstance(p, str) and re.fullmatch(r"P\d+", p.strip()):
            parts.append(p.strip())
            for f in fam_order:
                if f in fam_to_col:
                    data[f].append(to_float(raw.iat[r, fam_to_col[f]]))
                else:
                    data[f].append(0.0)
            streak = 0
        else:
            if parts:
                streak += 1
                if streak >= 40:
                    break

    bom = pd.DataFrame(data, index=parts).apply(pd.to_numeric, errors="coerce").fillna(0.0)
    bom = bom.loc[natural_sort(bom.index)]
    if float(bom.values.sum()) == 0.0:
        print("[WARN] BOM total is zero — please verify the BOM sheet.")
    return bom

# ---------------- Minutes per unit (fallback constants) ----------------
def minutes_series() -> pd.Series:
    """Fallback process minutes by part. Replace with real data if available."""
    steps = {
        "Part": [f"P{i}" for i in range(1, 21)],
        "Step 1": [2.5,1.25,1.75,1.0,1.5,0.75,1.0,1.25,1.75,1.5,1.25,1.0,1.25,1.0,0.75,1.25,0.75,0.75,2.25,2.0],
        "Step 2": [1.0,0.5,3.0,2.0,0.75,1.25,1.5,2.0,0.75,1.75,0.5,0.5,1.25,1.5,0.5,5.0,3.0,1.25,2.5,0.75],
        "Step 3": [2.5,2.5,0.75,3.0,3.5,0.5,0.75,0.5,1.25,1.25,1.25,1.0,0.5,0.5,1.25,1.25,3.5,0.5,2.0,3.0],
        "Step 4": [0.5,1.0,1.5,0.25,1.75,3.0,3.5,1.0,0.5,2.0,0.25,1.25,1.0,1.75,2.5,2.5,0.0,3.75,3.75,0.0],
        "Step 5": [2.5,2.5,2.5,1.25,0.0,1.0,1.25,0.0,1.25,0.0,0.75,2.25,0.25,0.0,2.5,0.0,0.0,0.0,0.0,0.0],
        "Step 6": [1.25,0.0,0.0,0.0,0.0,1.25,2.0,0.0,3.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        "Step 7": [2.5,0.0,0.0,0.0,0.0,2.75,0.0,0.0,0.0,0.0,0.0,0.0,1.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0]
    }
    df = pd.DataFrame(steps).set_index("Part")
    s = df.sum(axis=1).rename("Total_Minutes_Per_Unit")
    return s.loc[natural_sort(s.index)]

# ---------------- Compute per year ----------------
def compute_for_year(y: int, fam_order: List[str],
                     mean_by_year: Dict[str, pd.Series],
                     std_by_year: Dict[str, pd.Series],
                     bom: pd.DataFrame,
                     minutes_per_unit: pd.Series):
    """Compute per-part capacity numbers and write all CSVs for a single year."""
    tag = f"+{y}"
    out_dir = os.path.join(ROOT_OUT, f"Year+{y}")
    ensure_dir(out_dir)

    # Product (family) mean/std for the year
    prod_mean = mean_by_year[tag].astype(float).fillna(0.0)
    prod_std  = std_by_year[tag].astype(float).fillna(0.0)
    prod_var  = (prod_std ** 2).fillna(0.0)

    # Save product-level annual & weekly (to mirror Task1)
    prod_weekly_avg = (prod_mean / WEEKS_PER_YEAR)
    prod_weekly_std = (prod_std  / np.sqrt(WEEKS_PER_YEAR))
    pd.Series(prod_mean, name="Annual_Product_Mean").to_csv(os.path.join(out_dir, f"Product_Demand_Annual_Avg_Yp{y}.csv"))
    pd.Series(prod_std,  name="Annual_Product_Std").to_csv(os.path.join(out_dir, f"Product_Demand_Annual_StdDev_Yp{y}.csv"))
    prod_weekly_avg.rename("Weekly_Product_Mean").to_csv(os.path.join(out_dir, f"Product_Demand_Weekly_Avg_Yp{y}.csv"))
    prod_weekly_std.rename("Weekly_Product_StdDev").to_csv(os.path.join(out_dir, f"Product_Demand_Weekly_StdDev_Yp{y}.csv"))

    # Save BOM for this year folder (consistent with Task1 style)
    bom_out = bom.copy()
    bom_out.index.name = "Part"
    bom_out.to_csv(os.path.join(out_dir, f"Parts_Per_Product_BOM_Yp{y}.csv"))

    # Per-part annual mean & std from linear combinations
    s_mean = bom.dot(prod_mean).astype(float).fillna(0.0)
    s_std  = np.sqrt((bom ** 2).dot(prod_var)).astype(float).fillna(0.0)

    # Weekly per-part
    w_mean = (s_mean / WEEKS_PER_YEAR).astype(float).fillna(0.0)
    w_std  = (s_std  / np.sqrt(WEEKS_PER_YEAR)).astype(float).fillna(0.0)

    # Robust (units)
    a_rob_units = (s_mean + Z * s_std).astype(float).fillna(0.0)
    w_rob_units = (w_mean + Z * w_std).astype(float).fillna(0.0)

    # Minutes
    mins   = minutes_per_unit.reindex(s_mean.index).fillna(0.0)
    a_net   = (a_rob_units * mins).astype(float).fillna(0.0)
    a_gross = (a_net / (EFFICIENCY * RELIABILITY)).astype(float).fillna(0.0)
    w_net   = (w_rob_units * mins).astype(float).fillna(0.0)
    w_gross = (w_net / (EFFICIENCY * RELIABILITY)).astype(float).fillna(0.0)

    # Natural part order
    parts_order = natural_sort([str(p) for p in s_mean.index])

    # ---- FULL TABLE (per-part totals) ----
    df_full = pd.DataFrame({
        "Annual_Demand_Mean_Units": s_mean,
        "Annual_Demand_StdDev_Units": s_std,
        "Weekly_Demand_Mean_Units": w_mean,
        "Weekly_Demand_StdDev_Units": w_std,
        "Annual_Robust_Demand_99p5_Units": a_rob_units,
        "Annual_Robust_Net_Minutes": a_net,
        "Annual_Robust_Gross_Minutes": a_gross,
        "Weekly_Robust_Demand_99p5_Units": w_rob_units,
        "Weekly_Robust_Net_Minutes": w_net,
        "Weekly_Robust_Gross_Minutes": w_gross,
        "Total_Minutes_Per_Unit": mins
    })
    df_full.index.name = "Part"
    df_full = df_full.loc[parts_order]

    # Paths
    full_out    = os.path.join(out_dir, f"Task4_Full_Capacity_Annual_Weekly_Robust_Yp{y}.csv")
    weekly_out  = os.path.join(out_dir, f"Task4_Per_Part_Weekly_Mean_Std_Robust_Yp{y}.csv")
    annual_out  = os.path.join(out_dir, f"Task4_Per_Part_Annual_Mean_Std_Robust_Yp{y}.csv")

    # Write full
    df_full.to_csv(full_out)

    # ---- WEEKLY SLICE ----
    df_weekly = df_full[[
        "Weekly_Demand_Mean_Units",
        "Weekly_Demand_StdDev_Units",
        "Weekly_Robust_Demand_99p5_Units",
        "Weekly_Robust_Net_Minutes",
        "Weekly_Robust_Gross_Minutes"
    ]]
    df_weekly.to_csv(weekly_out)

    # ---- ANNUAL SLICE ----
    df_annual = df_full[[
        "Annual_Demand_Mean_Units",
        "Annual_Demand_StdDev_Units",
        "Annual_Robust_Demand_99p5_Units",
        "Annual_Robust_Net_Minutes",
        "Annual_Robust_Gross_Minutes"
    ]]
    df_annual.to_csv(annual_out)

    # ---- Parts-by-family demand (Annual & Weekly), Task1-style means ----
    parts_by_fam_annual = bom.mul(prod_mean, axis=1)
    parts_by_fam_annual["Total Demand (Annual Avg)"] = parts_by_fam_annual.sum(axis=1)
    parts_by_fam_weekly = bom.mul(prod_weekly_avg, axis=1)
    parts_by_fam_weekly["Total Demand (Weekly Avg)"] = parts_by_fam_weekly.sum(axis=1)
    parts_by_fam_annual = parts_by_fam_annual.loc[parts_order]
    parts_by_fam_weekly = parts_by_fam_weekly.loc[parts_order]
    parts_by_fam_annual.to_csv(os.path.join(out_dir, f"Parts_Total_Demand_By_Family_AnnualAvg_Yp{y}.csv"))
    parts_by_fam_weekly.to_csv(os.path.join(out_dir, f"Parts_Total_Demand_By_Family_WeeklyAvg_Yp{y}.csv"))

    # ---- Parts×Family Weekly ROBUST (Units & Gross Minutes) — CONSISTENT WITH FULL ----
    fam_weekly_mean = prod_weekly_avg
    fam_weekly_std  = prod_weekly_std
    fam_weekly_var  = fam_weekly_std ** 2

    # Mean & variance contribution matrices by family
    mean_mat = bom.mul(fam_weekly_mean, axis=1)                    # (parts x families)
    var_mat  = (bom ** 2).mul(fam_weekly_var, axis=1)              # (parts x families)

    # Per-part totals (same as df_full weekly)
    M_part   = mean_mat.sum(axis=1)
    std_part = np.sqrt(var_mat.sum(axis=1))

    # Allocate z*std_part back to families by std contribution weights
    std_contrib = np.sqrt(var_mat).fillna(0.0)                     # sqrt of per-family var contrib
    denom = std_contrib.sum(axis=1).replace(0.0, np.nan)
    weights = std_contrib.div(denom, axis=0).fillna(0.0)

    addend = (Z * std_part).rename("ZStd")                         # per part vector
    addend_mat = weights.mul(addend, axis=0)
    robust_units_mat = mean_mat + addend_mat                       # per-part-family robust units

    # Robust Gross Minutes per part-family
    robust_gross_minutes_mat = robust_units_mat.mul(mins, axis=0) / (EFFICIENCY * RELIABILITY)

    # Export the combined table
    out_units = robust_units_mat.copy()
    out_minutes = robust_gross_minutes_mat.copy()
    out_minutes.columns = [f"{c}_GrossMin" for c in out_minutes.columns]

    combo = pd.concat([out_units, out_minutes], axis=1)
    combo["Total Weekly Robust Units"] = out_units.sum(axis=1)
    combo["Total Weekly Robust Gross Minutes"] = out_minutes.sum(axis=1)
    combo = combo.loc[parts_order]
    combo.to_csv(os.path.join(out_dir, f"Parts_By_Family_Weekly_Robust_Units_and_GrossMinutes_Yp{y}.csv"))

    # ---- AUDIT: Reconcile per-part sums with FULL ----
    row_sum_units = out_units.sum(axis=1)
    row_sum_gross = out_minutes.sum(axis=1)
    full_units = w_rob_units.rename("Full_Weekly_Robust_Units")
    full_gross = w_gross.rename("Full_Weekly_Robust_Gross_Minutes")

    audit = pd.concat([
        row_sum_units.rename("ByFamily_RowSum_Units"),
        full_units,
        (row_sum_units - full_units).rename("Diff_Units"),
        row_sum_gross.rename("ByFamily_RowSum_GrossMin"),
        full_gross,
        (row_sum_gross - full_gross).rename("Diff_GrossMin")
    ], axis=1).loc[parts_order]
    audit_out = os.path.join(out_dir, f"Task4_Audit_Reconciliation_Yp{y}.csv")
    audit.to_csv(audit_out)

    tol_units = float(audit["Diff_Units"].abs().max())
    tol_gross = float(audit["Diff_GrossMin"].abs().max())
    summary_txt = os.path.join(out_dir, f"Task4_Audit_Summary_Yp{y}.txt")
    with open(summary_txt, "w", encoding="utf-8") as f:
        f.write("Task4 per-part reconciliation (By-Family vs FULL)\n")
        f.write(f"Max |Diff_Units|     = {tol_units:.6g}\n")
        f.write(f"Max |Diff_GrossMin|  = {tol_gross:.6g}\n")
        f.write("Note: values ~1e-12..1e-6 indicate floating-point round-off only.\n")

    # Transparency only: compare family column sums (allocation vs naive family-first robust)
    naive_units = bom.mul(fam_weekly_mean + Z * fam_weekly_std, axis=1)
    family_alloc_totals = out_units.sum(axis=0).rename("Allocated_Robust_Units_ColSum")
    family_naive_totals = naive_units.sum(axis=0).rename("Naive_Robust_Units_ColSum")
    pd.concat([family_alloc_totals, family_naive_totals], axis=1).to_csv(
        os.path.join(out_dir, f"Task4_Audit_FamilyColumnSums_Yp{y}.csv")
    )

    # Console diagnostics
    with np.errstate(divide='ignore', invalid='ignore'):
        ratio = (std_by_year[tag] / prod_mean.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan)
    med_ratio = float(np.nanmedian(ratio.values))
    print(f"[CHECK] {tag}: parts={len(df_full)} | sum(product means)={float(prod_mean.sum()):.0f} | median(std/mean)={med_ratio:.3f}")
    print(f"[AUDIT] Max |Diff Units|={tol_units:.6g} | Max |Diff GrossMin|={tol_gross:.6g}")
    print(f"✅ Year +{y}: wrote outputs to {out_dir}")

# ---------------- Entrypoint ----------------
def main():
    if not Path(EXCEL_PATH).exists():
        raise FileNotFoundError(f"Excel not found: {EXCEL_PATH}")
    ensure_dir(ROOT_OUT)

    fam_order, mean_by_year, std_by_year = read_demand_mean_std_or_cv(EXCEL_PATH)
    bom = read_bom_consolidated(EXCEL_PATH, fam_order)
    minutes = minutes_series()

    # Align BOM columns to the unified family order
    if list(bom.columns) != list(fam_order):
        bom = bom.reindex(columns=fam_order).fillna(0.0)
        print("[WARN] BOM columns re-aligned to demand family order.")

    for y in YEARS:
        compute_for_year(y, fam_order, mean_by_year, std_by_year, bom, minutes)

    print(f"\nAll done. See: {os.path.abspath(ROOT_OUT)}")

if __name__ == "__main__":
    main()
