# -*- coding: utf-8 -*-
"""
ISyE 6202 & 6335 - Casework 3
Task 4: Multi-Year Capacity Plan (Years +2..+5) — Robust, No-NaN, Syntax-Fixed

- Reads consolidated sheets (“+2 to +5 Year Product Demand”, “+2 to +5 Year Parts per Product”)
  by scanning cells (header=None), not relying on exact pandas headers.
- Strong numeric parsing (commas, spaces, parentheses negatives).
- If std-block missing/partial => std treated as 0 (never NaN).
- After each stage, fillna(0) so CSV不会再出现空白列。
"""

import os
import re
import math
from pathlib import Path
from typing import Dict, List, Tuple
import numpy as np
import pandas as pd
from scipy.stats import norm

# ---------------- Tunables ----------------
EFFICIENCY = 0.90
RELIABILITY = 0.98
SERVICE_LEVEL = 0.995
Z = norm.ppf(SERVICE_LEVEL)
WEEKS_PER_YEAR = 52.0
YEARS = [2, 3, 4, 5]

EXCEL_PATH = "iSYE 6202 & 6335 2025 Casework 3 FaMoaSa Facility Design - Tables and Basic Layouts.xlsx"
ROOT_OUT = "Task4_Outcome"

FAM_RE   = re.compile(r"^[AB]\d+$")
YEAR_TAG = {f"+{y}" for y in YEARS}

# -------------- Utilities ---------------
def ensure_dir(p: str) -> None:
    os.makedirs(p, exist_ok=True)

def natural_sort(labels: List[str]) -> List[str]:
    def key(s):
        s = str(s)
        num = "".join(ch for ch in s if ch.isdigit())
        return (s.rstrip(num), int(num) if num else math.inf)
    return sorted(labels, key=key)

def to_float(x) -> float:
    """Robust numeric cast for Excel-like content."""
    if isinstance(x, (int, float, np.integer, np.floating)) and not pd.isna(x):
        return float(x)
    if x is None:
        return np.nan
    s = str(x).strip()
    if s == "" or s.lower() in {"na","nan","none","-"}:
        return np.nan
    neg = False
    if s.startswith("(") and s.endswith(")"):
        neg = True
        s = s[1:-1]
    s = s.replace(",", "").strip()
    m = re.match(r"^[+-]?\d+(\.\d+)?", s)
    if not m:
        return np.nan
    v = float(m.group(0))
    return -v if neg else v

# -------- Demand (consolidated) ----------
def read_consolidated_demand(xl_path: str) -> Tuple[List[str], Dict[str, pd.Series], Dict[str, pd.Series]]:
    xls = pd.ExcelFile(xl_path)
    sheet = None
    for cand in ["+2 to +5 Year Product Demand ", "+2 to +5 Year Product Demand"]:
        if cand in xls.sheet_names:
            sheet = cand
            break
    if sheet is None:
        raise ValueError("Consolidated '+2 to +5 Year Product Demand' sheet not found.")

    raw = pd.read_excel(xl_path, sheet_name=sheet, header=None)
    nrows, ncols = raw.shape

    # 1) locate family header row
    header = None
    fam_pos = []
    for r in range(min(80, nrows)):
        found = []
        for c in range(ncols):
            v = raw.iat[r, c]
            if isinstance(v, str) and FAM_RE.match(v.strip()):
                found.append((v.strip(), c))
        if len({f for f, _ in found}) >= 4:
            header = r
            fam_pos = found
            break
    if header is None:
        raise ValueError("Family header row not found in demand sheet.")

    fam_list = sorted({f for f, _ in fam_pos}, key=lambda x: (x[0], int(x[1:])))
    fam_to_col = {f: c for f, c in fam_pos if f in fam_list}

    # 2) locate Year column under header
    year_col = None
    for c in range(ncols):
        vals = [raw.iat[r, c] for r in range(header + 1, min(header + 15, nrows))]
        if sum(1 for v in vals if isinstance(v, str) and v.strip() in YEAR_TAG) >= 2:
            year_col = c
            break
    if year_col is None:
        for c in range(min(6, ncols)):
            vals = [raw.iat[r, c] for r in range(header + 1, min(header + 15, nrows))]
            if any(isinstance(v, str) and v.strip() in YEAR_TAG for v in vals):
                year_col = c
                break
    if year_col is None:
        raise ValueError("Year column not found under demand header.")

    # 3) mean rows (+2..+5)
    mean_rows = {}
    for r in range(header + 1, min(header + 25, nrows)):
        v = raw.iat[r, year_col]
        if isinstance(v, str) and v.strip() in YEAR_TAG:
            mean_rows[v.strip()] = r
    if not mean_rows:
        raise ValueError("No +2..+5 rows found in demand mean block.")

    # 4) std block (optional)
    std_title = None
    for r in range(header + 1, min(header + 60, nrows)):
        txt = " ".join(str(x) for x in raw.iloc[r, :min(12, ncols)].tolist() if pd.notna(x))
        if "Standard Deviation" in txt:
            std_title = r
            break

    std_rows = {}
    if std_title is not None:
        header_std = None
        for r in range(std_title + 1, min(std_title + 15, nrows)):
            found = []
            for c in range(ncols):
                v = raw.iat[r, c]
                if isinstance(v, str) and FAM_RE.match(v.strip()):
                    found.append((v.strip(), c))
            if len({f for f, _ in found}) >= 4:
                header_std = r
                break
        year_col_std = year_col
        if header_std is not None:
            ok = any(
                isinstance(raw.iat[r, year_col_std], str) and raw.iat[r, year_col_std].strip() in YEAR_TAG
                for r in range(header_std + 1, min(header_std + 12, nrows))
            )
            if not ok:
                for c in range(ncols):
                    vals = [raw.iat[r, c] for r in range(header_std + 1, min(header_std + 12, nrows))]
                    if any(isinstance(v, str) and v.strip() in YEAR_TAG for v in vals):
                        year_col_std = c
                        break
            for r in range(header_std + 1, min(header_std + 25, nrows)):
                v = raw.iat[r, year_col_std]
                if isinstance(v, str) and v.strip() in YEAR_TAG:
                    std_rows[v.strip()] = r

    mean_by_year: Dict[str, pd.Series] = {}
    std_by_year:  Dict[str, pd.Series] = {}

    for y in YEARS:
        tag = f"+{y}"
        if tag not in mean_rows:
            raise ValueError(f"Mean row missing for {tag}")
        r = mean_rows[tag]
        mean_vals = [to_float(raw.iat[r, fam_to_col[f]]) for f in fam_list]
        mean_by_year[tag] = pd.Series(mean_vals, index=fam_list, dtype="float64").fillna(0.0)

        if tag in std_rows:
            rs = std_rows[tag]
            std_vals = [to_float(raw.iat[rs, fam_to_col[f]]) for f in fam_list]
            std_by_year[tag] = pd.Series(std_vals, index=fam_list, dtype="float64").fillna(0.0)
        else:
            std_by_year[tag] = pd.Series([0.0] * len(fam_list), index=fam_list, dtype="float64")

    return fam_list, mean_by_year, std_by_year

# -------------- BOM (consolidated) --------------
def read_consolidated_bom(xl_path: str, fam_list: List[str]) -> pd.DataFrame:
    xls = pd.ExcelFile(xl_path)
    sheet = None
    for cand in ["+2 to +5 Year Parts per Product", "+2 to +5 Year Parts per Product "]:
        if cand in xls.sheet_names:
            sheet = cand
            break
    if sheet is None:
        raise ValueError("Consolidated '+2 to +5 Year Parts per Product' sheet not found.")

    raw = pd.read_excel(xl_path, sheet_name=sheet, header=None)
    nrows, ncols = raw.shape

    # detect Part column by Pxx hits
    part_col = None
    best = -1
    for c in range(min(25, ncols)):
        hits = raw.iloc[:min(200, nrows), c].astype(str).str.fullmatch(r"P\d+").sum()
        if hits > best:
            best = hits
            part_col = c
    if part_col is None or best < 5:
        raise ValueError("BOM: 'Part' column not detected.")

    # map family -> column index by scanning top region
    fam_to_col = {}
    for r in range(min(40, nrows)):
        for c in range(ncols):
            v = raw.iat[r, c]
            if isinstance(v, str) and v.strip() in fam_list and v.strip() not in fam_to_col:
                fam_to_col[v.strip()] = c

    missing = [f for f in fam_list if f not in fam_to_col]
    if missing:
        raise ValueError(f"BOM is missing family columns: {missing}")

    parts = []
    data = {f: [] for f in fam_list}
    for r in range(nrows):
        p = raw.iat[r, part_col]
        if isinstance(p, str) and re.fullmatch(r"P\d+", p.strip()):
            parts.append(p.strip())
            for f in fam_list:
                data[f].append(to_float(raw.iat[r, fam_to_col[f]]))

    bom = pd.DataFrame(data, index=parts).apply(pd.to_numeric, errors="coerce").fillna(0.0)
    bom = bom.loc[natural_sort(list(bom.index))]
    if float(bom.values.sum()) == 0.0:
        print("[WARN] BOM sums to 0. Check sheet contents.")
    return bom

# -------- Minutes per unit (fallback) --------
def minutes_series() -> pd.Series:
    steps = {
        "Part": [f"P{i}" for i in range(1, 21)],
        "Step 1": [2.5,1.25,1.75,1.0,1.5,0.75,1.0,1.25,1.75,1.5,1.25,1.0,1.25,1.0,0.75,1.25,0.75,0.75,2.25,2.0],
        "Step 2": [1.0,0.5,3.0,2.0,0.75,1.25,1.5,2.0,0.75,1.75,0.5,0.5,1.25,1.5,0.5,5.0,3.0,1.25,2.5,0.75],
        "Step 3": [2.5,2.5,0.75,3.0,3.5,0.5,0.75,0.5,1.25,1.25,1.25,1.0,0.5,0.5,1.25,1.25,3.5,0.5,2.0,3.0],
        "Step 4": [0.5,1.0,1.5,0.25,1.75,3.0,3.5,1.0,0.5,2.0,0.25,1.25,1.0,1.75,2.5,2.5,0.0,3.75,3.75,0.0],
        "Step 5": [2.5,2.5,2.5,1.25,0.0,1.0,1.25,0.0,1.25,0.0,0.75,2.25,0.25,0.0,2.5,0.0,0.0,0.0,0.0,0.0],
        "Step 6": [1.25,0.0,0.0,0.0,0.0,1.25,2.0,0.0,3.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],
        "Step 7": [2.5,0.0,0.0,0.0,0.0,2.75,0.0,0.0,0.0,0.0,0.0,0.0,1.25,0.0,0.0,0.0,0.0,0.0,0.0,0.0]
    }
    df = pd.DataFrame(steps).set_index("Part")
    s = df.sum(axis=1).rename("Total_Minutes_Per_Unit")
    # >>> fixed bracket here <<<
    return s.loc[natural_sort(list(s.index))]

# ---------------- Core compute ----------------
def compute_year(y: int, fam_list: List[str],
                 mean_by_year: Dict[str, pd.Series],
                 std_by_year: Dict[str, pd.Series],
                 bom: pd.DataFrame,
                 minutes_per_unit: pd.Series):
    tag = f"+{y}"
    out_dir = os.path.join(ROOT_OUT, f"Year+{y}")
    ensure_dir(out_dir)

    prod_mean = mean_by_year[tag].astype(float).fillna(0.0)
    prod_std  = std_by_year[tag].astype(float).fillna(0.0)
    prod_var  = (prod_std ** 2).fillna(0.0)

    # per-part annual mean/std
    s_mean = bom.dot(prod_mean).astype(float).fillna(0.0)
    s_std  = np.sqrt((bom ** 2).dot(prod_var)).astype(float).fillna(0.0)

    # weekly
    w_mean = (s_mean / WEEKS_PER_YEAR).astype(float).fillna(0.0)
    w_std  = (s_std  / np.sqrt(WEEKS_PER_YEAR)).astype(float).fillna(0.0)

    # robust units
    a_rob_units = (s_mean + Z * s_std).astype(float).fillna(0.0)
    w_rob_units = (w_mean + Z * w_std).astype(float).fillna(0.0)

    # minutes
    mins = minutes_per_unit.reindex(s_mean.index).fillna(0.0)
    a_net   = (a_rob_units * mins).astype(float).fillna(0.0)
    a_gross = (a_net / (EFFICIENCY * RELIABILITY)).astype(float).fillna(0.0)
    w_net   = (w_rob_units * mins).astype(float).fillna(0.0)
    w_gross = (w_net / (EFFICIENCY * RELIABILITY)).astype(float).fillna(0.0)

    # final table (no NaNs)
    df_full = pd.DataFrame({
        "Part": s_mean.index,
        "Annual_Demand_Mean_Units": s_mean.values,
        "Annual_Demand_StdDev_Units": s_std.values,
        "Weekly_Demand_Mean_Units": w_mean.values,
        "Weekly_Demand_StdDev_Units": w_std.values,
        "Annual_Robust_Demand_99p5_Units": a_rob_units.values,
        "Annual_Robust_Net_Minutes": a_net.values,
        "Annual_Robust_Gross_Minutes": a_gross.values,
        "Weekly_Robust_Demand_99p5_Units": w_rob_units.values,
        "Weekly_Robust_Net_Minutes": w_net.values,
        "Weekly_Robust_Gross_Minutes": w_gross.values,
        "Total_Minutes_Per_Unit": mins.values
    })

    # sort by P-number
    def sort_key(p):
        s = str(p)
        num = "".join(ch for ch in s if ch.isdigit())
        return (s.rstrip(num), int(num) if num else math.inf)

    df_full = df_full.loc[df_full["Part"].astype(str).str.match(r"^P\d+$")]
    df_full = df_full.sort_values("Part", key=lambda col: col.map(sort_key))

    # write
    full_out   = os.path.join(out_dir, f"Task4_Full_Capacity_Annual_Weekly_Robust_Yp{y}.csv")
    weekly_out = os.path.join(out_dir, f"Task4_Per_Part_Weekly_Mean_Std_Robust_Yp{y}.csv")
    df_full.to_csv(full_out, index=False)
    df_full[["Part",
             "Weekly_Demand_Mean_Units",
             "Weekly_Demand_StdDev_Units",
             "Weekly_Robust_Demand_99p5_Units",
             "Weekly_Robust_Net_Minutes",
             "Weekly_Robust_Gross_Minutes"]].to_csv(weekly_out, index=False)

    # traceability
    pd.Series(prod_mean, name="Annual_Product_Mean").to_csv(os.path.join(out_dir, f"Product_Demand_Annual_Avg_Yp{y}.csv"))
    pd.Series(prod_std,  name="Annual_Product_Std").to_csv(os.path.join(out_dir, f"Product_Demand_Annual_StdDev_Yp{y}.csv"))
    bom.reset_index().rename(columns={"index": "Part"}).to_csv(os.path.join(out_dir, f"Parts_Per_Product_BOM_Yp{y}.csv"), index=False)
    (bom.mul(prod_mean, axis=1)
        .assign(**{"Total Demand (Annual Avg)": lambda d: d.sum(axis=1)})
        .to_csv(os.path.join(out_dir, f"Parts_Total_Demand_By_Family_AnnualAvg_Yp{y}.csv")))

    print(f"✅ Year +{y}: wrote outputs to {out_dir}")

# ---------------- Entrypoint ----------------
def main():
    if not Path(EXCEL_PATH).exists():
        raise FileNotFoundError(f"Excel not found: {EXCEL_PATH}")
    ensure_dir(ROOT_OUT)

    fam_list, mean_by_year, std_by_year = read_consolidated_demand(EXCEL_PATH)
    bom = read_consolidated_bom(EXCEL_PATH, fam_list)
    minutes = minutes_series()

    for y in YEARS:
        compute_year(y, fam_list, mean_by_year, std_by_year, bom, minutes)

    print(f"All done. See: {os.path.abspath(ROOT_OUT)}")

if __name__ == "__main__":
    main()
