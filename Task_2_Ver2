# -*- coding: utf-8 -*-
"""
ISyE 6202 & 6335 - Casework 3 - Task 2
Finished Goods Storage Capacity Plan + Cadence Self-Check + Plots

Outputs (CSV/PNG/TXT) -> ./Task2_Outcome/
1) task2_storage_plan_by_part.csv
2) task2_physical_storage_summary.csv
3) task2_cadence_selfcheck_by_part.csv
4) README_Task2_Methodology.txt
5) plots:
   - plot_footprint_vs_utilization.png
   - plot_cadence_coverage_A.png
   - plot_cadence_coverage_B.png
   - plot_top10_risk_parts_A.png
   - plot_top10_risk_parts_B.png
"""

import os
import numpy as np
import pandas as pd
from math import sqrt
from scipy.stats import norm
import matplotlib.pyplot as plt

# =========================
# Config / Parameters
# =========================
EXCEL_PATH = "iSYE+6202+%26+6335+2025+Casework+3+FaMoaSa++Facility+Design+-+Tables+and+Basic+Layouts.xlsx"
OUT_DIR    = "Task2_Outcome"

DEMAND_SHEET  = "+1 Year Product Demand"
BOM_SHEET     = "+1 Year Parts per Product"
HEADER_DEMAND = 16
HEADER_BOM    = 9

# Demand uses 52 weeks (customer consumption); production availability (49) is Task 1 side
WEEKS_PER_YEAR_DEMAND = 52.0

# OTIF target ‚Üí service level
SERVICE_LEVEL = 0.995
Z = float(norm.ppf(SERVICE_LEVEL))  # ‚âà 2.576

# Supply policy (decoupled)
LEAD_TIME_WEEKS    = 1.0   # L, for SS
REVIEW_CYCLE_WEEKS = 1.0   # T, for CS

# Factory outbound buffer (weeks of total avg demand)
FACTORY_BUFFER_WEEKS = 2.0

# Space & cost (sensitivity on utilization)
SPACE_UTIL_FACTORS        = [0.35, 0.40, 0.45]
NEAR_CLIENT_HEIGHT_FT     = 20.0
FACTORY_HEIGHT_FT         = 20.0
NEAR_CLIENT_COST_PER_SQFT = 200.0

# Operations hours assumptions (for cadence self-check)
CLIENT_OPER_DAYS_PER_WEEK   = 5
CLIENT_OPER_HOURS_PER_DAY   = 16   # e.g., 2 shifts √ó 8 h
HOURS_PER_WEEK_A = CLIENT_OPER_DAYS_PER_WEEK * CLIENT_OPER_HOURS_PER_DAY
HOURS_PER_WEEK_B = CLIENT_OPER_DAYS_PER_WEEK * CLIENT_OPER_HOURS_PER_DAY

CLIENT_A_WINDOW_HOURS = 1.0  # hourly
CLIENT_B_WINDOW_HOURS = 4.0  # every 4 hours

# =========================
# Helpers: parse demand (+1)
# =========================
def parse_plus1_product_demand_safe(path: str):
    """
    Robustly extract +1 year product annual mean (A1..B2) and weekly mean.
    The sheet has multiple '+1' blocks (annual / weekly / CoV); pick row with max sum as annual mean.
    """
    products = ["A1","A2","A3","B1","B2"]
    df = pd.read_excel(path, sheet_name=DEMAND_SHEET, header=HEADER_DEMAND)
    plus1 = df[df["Year"].astype(str) == "+1"].copy()
    if plus1.empty:
        raise ValueError("Cannot locate Year '+1' in demand sheet.")
    def sum_products(row):
        return float(pd.to_numeric(row[products], errors="coerce").sum())
    plus1["sumv"] = plus1.apply(sum_products, axis=1)
    annual_row = plus1.sort_values("sumv", ascending=False).iloc[0]
    annual_mean = pd.to_numeric(annual_row[products], errors="coerce").astype(float)
    weekly_mean = annual_mean / WEEKS_PER_YEAR_DEMAND
    return annual_mean, weekly_mean

# =========================
# Helpers: BOM
# =========================
def read_bom(path: str) -> pd.DataFrame:
    raw = pd.read_excel(path, sheet_name=BOM_SHEET, header=HEADER_BOM)
    df = raw.rename(columns={
        "Parts per Assembled Product Unit Demanded in Year +1 ": "A1",
        "Unnamed: 3": "A2",
        "Unnamed: 4": "A3",
        "Unnamed: 5": "B1",
        "Unnamed: 6": "B2",
    })
    df = df[["Part","A1","A2","A3","B1","B2"]].dropna(subset=["Part"]).reset_index(drop=True)
    for c in ["A1","A2","A3","B1","B2"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0.0)
    return df.set_index("Part")

# =========================
# Manual: Parts Dimensions (from PDF)
# =========================
def get_manual_specs_dimensions():
    specs_data = {
        'Part': ['P1','P2','P3','P4','P5','P6','P7','P8','P9','P10',
                 'P11','P12','P13','P14','P15','P16','P17','P18','P19','P20'],
        'X': [2,8,6,12,8,2,2,4,2,4,4,6,2,2,4,4,12,12,12,12],
        'Y': [6,8,6,6,4,8,2,4,4,4,6,6,2,4,6,4,2,2,2,2],
        'Z': [6,4,6,4,6,6,12,4,12,4,4,4,12,6,4,4,2,2,2,2]
    }
    df = pd.DataFrame(specs_data).set_index("Part")
    df["Volume_CF"] = (df["X"]*df["Y"]*df["Z"]) / 1728.0
    return df[["Volume_CF"]]

# =========================
# Stats propagation: products ‚Üí parts
# =========================
def compute_weekly_part_stats(bom: pd.DataFrame,
                              weekly_mean_prod: pd.Series,
                              weekly_cov_prod: pd.Series):
    """
    Propagate weekly mean/variance from products to parts via BOM.
    Assume product independence (variance adds).
    """
    products_A = ["A1","A2","A3"]
    products_B = ["B1","B2"]

    s_avg_A = bom[products_A].dot(weekly_mean_prod[products_A])
    s_avg_B = bom[products_B].dot(weekly_mean_prod[products_B])
    s_avg_total = s_avg_A + s_avg_B

    weekly_var_prod = (weekly_cov_prod * weekly_mean_prod)**2
    s_var_A = (bom[products_A]**2).dot(weekly_var_prod[products_A])
    s_var_B = (bom[products_B]**2).dot(weekly_var_prod[products_B])

    s_std_A = np.sqrt(s_var_A)
    s_std_B = np.sqrt(s_var_B)

    return s_avg_A, s_avg_B, s_avg_total, s_std_A, s_std_B

# =========================
# Inventory policies
# =========================
def compute_stocks(s_avg_A, s_avg_B, s_avg_total, s_std_A, s_std_B,
                   z: float,
                   lead_time_weeks: float,
                   review_cycle_weeks: float):
    """
    SS/CS for Warehouse A & B; factory outbound buffer for decoupling.
    """
    ss_A = z * sqrt(lead_time_weeks) * s_std_A
    ss_B = z * sqrt(lead_time_weeks) * s_std_B

    cs_A = (s_avg_A * review_cycle_weeks) / 2.0
    cs_B = (s_avg_B * review_cycle_weeks) / 2.0

    wh_A = ss_A + cs_A
    wh_B = ss_B + cs_B

    factory_buf = s_avg_total * FACTORY_BUFFER_WEEKS

    df_plan = pd.DataFrame({
        "Avg_Weekly_Demand_A": s_avg_A,
        "Safety_Stock_A": ss_A,
        "Cycle_Stock_A": cs_A,
        "Total_Warehouse_A_Stock": wh_A,
        "Avg_Weekly_Demand_B": s_avg_B,
        "Safety_Stock_B": ss_B,
        "Cycle_Stock_B": cs_B,
        "Total_Warehouse_B_Stock": wh_B,
        "Factory_Outbound_Stock": factory_buf,
        "Total_System_Inventory": wh_A + wh_B + factory_buf
    })

    # round up to integers for stock quantities
    cols_int = [
        "Safety_Stock_A","Cycle_Stock_A","Total_Warehouse_A_Stock",
        "Safety_Stock_B","Cycle_Stock_B","Total_Warehouse_B_Stock",
        "Factory_Outbound_Stock","Total_System_Inventory"
    ]
    df_plan[cols_int] = np.ceil(df_plan[cols_int]).astype(int)

    return df_plan

# =========================
# Space / Cost summary
# =========================
def physical_summary(df_plan_with_vol: pd.DataFrame,
                     space_utils: list,
                     near_client_height: float,
                     factory_height: float,
                     near_client_cost_per_sqft: float):
    vol_A = float((df_plan_with_vol["Total_Warehouse_A_Stock"] * df_plan_with_vol["Volume_CF"]).sum())
    vol_B = float((df_plan_with_vol["Total_Warehouse_B_Stock"] * df_plan_with_vol["Volume_CF"]).sum())
    vol_F = float((df_plan_with_vol["Factory_Outbound_Stock"] * df_plan_with_vol["Volume_CF"]).sum())

    rows = []
    for util in space_utils:
        sqft_A = (vol_A / util) / near_client_height if util > 0 else np.nan
        sqft_B = (vol_B / util) / near_client_height if util > 0 else np.nan
        sqft_F = (vol_F / util) / factory_height     if util > 0 else np.nan

        cost_A = sqft_A * near_client_cost_per_sqft
        cost_B = sqft_B * near_client_cost_per_sqft

        rows += [
            {"Space_Utilization": util, "Location": "Warehouse A", "Purpose": "Service Client A",
             "Total_Storage_Volume_CF": vol_A, "Required_Sq_Footage": sqft_A, "Est_Building_Cost": cost_A},
            {"Space_Utilization": util, "Location": "Warehouse B", "Purpose": "Service Client B",
             "Total_Storage_Volume_CF": vol_B, "Required_Sq_Footage": sqft_B, "Est_Building_Cost": cost_B},
            {"Space_Utilization": util, "Location": "Factory Outbound", "Purpose": "Replenish Warehouses",
             "Total_Storage_Volume_CF": vol_F, "Required_Sq_Footage": sqft_F, "Est_Building_Cost": np.nan}
        ]
    return pd.DataFrame(rows)

# =========================
# Cadence self-check (hourly / 4-hour)
# =========================
def cadence_selfcheck(df_plan: pd.DataFrame,
                      s_avg_A: pd.Series, s_avg_B: pd.Series,
                      s_std_A: pd.Series, s_std_B: pd.Series,
                      z: float,
                      hours_per_week_A: float,
                      hours_per_week_B: float,
                      win_A_hours: float,
                      win_B_hours: float):
    """
    Scale weekly stats to short windows:
      Œº_hour = Œº_week / hours_per_week
      œÉ_hour = œÉ_week / sqrt(hours_per_week)
      Need_win = Œº_hour * h + z * œÉ_hour * sqrt(h)
    Conservative flags:
      PASS_BY_SS, PASS_BY_SS+CS, RISK_WINDOW_SHORTFALL
    """
    df = pd.DataFrame(index=df_plan.index)

    # A window
    mu_h_A  = s_avg_A / hours_per_week_A
    sig_h_A = s_std_A / sqrt(hours_per_week_A)
    need_A  = mu_h_A * win_A_hours + z * sig_h_A * sqrt(win_A_hours)

    ss_A = df_plan["Safety_Stock_A"].astype(float)
    cs_A = df_plan["Cycle_Stock_A"].astype(float)

    df["A_Window_Need"] = need_A
    df["A_SS_Coverage_Ratio"] = ss_A / need_A.replace(0, np.nan)
    df["A_SS_Pass"] = (ss_A >= need_A)
    df["A_SSplusCS_Coverage_Ratio"] = (ss_A + cs_A) / need_A.replace(0, np.nan)
    df["A_SSplusCS_Pass"] = ((ss_A + cs_A) >= need_A)
    df["A_Flag"] = np.where(df["A_SS_Pass"], "PASS_BY_SS",
                    np.where(df["A_SSplusCS_Pass"], "PASS_BY_SS+CS", "RISK_WINDOW_SHORTFALL"))

    # B window
    mu_h_B  = s_avg_B / hours_per_week_B
    sig_h_B = s_std_B / sqrt(hours_per_week_B)
    need_B  = mu_h_B * win_B_hours + z * sig_h_B * sqrt(win_B_hours)

    ss_B = df_plan["Safety_Stock_B"].astype(float)
    cs_B = df_plan["Cycle_Stock_B"].astype(float)

    df["B_Window_Need"] = need_B
    df["B_SS_Coverage_Ratio"] = ss_B / need_B.replace(0, np.nan)
    df["B_SS_Pass"] = (ss_B >= need_B)
    df["B_SSplusCS_Coverage_Ratio"] = (ss_B + cs_B) / need_B.replace(0, np.nan)
    df["B_SSplusCS_Pass"] = ((ss_B + cs_B) >= need_B)
    df["B_Flag"] = np.where(df["B_SS_Pass"], "PASS_BY_SS",
                    np.where(df["B_SSplusCS_Pass"], "PASS_BY_SS+CS", "RISK_WINDOW_SHORTFALL"))

    return df

# =========================
# README writer
# =========================
def write_readme(path_txt: str):
    with open(path_txt, "w", encoding="utf-8") as f:
        f.write(
f"""Task 2: Finished Storage Capacity Plan - Methodology
=====================================================

Goal:
- Build part-level inventory plan for Warehouse A (Client A), Warehouse B (Client B), and Factory Outbound buffer.
- Convert inventory to physical volume, footprint, and near-client building cost (with utilization sensitivity).
- Add hourly / 4-hour cadence self-check to validate short-window OTIF feasibility.

Key formulas (weekly scale):
- Œº_week = Œº_annual / {WEEKS_PER_YEAR_DEMAND}
- Var_week = (Œº_week * CoV)^2  (assume independence across products)
- SS = z * sqrt(L) * œÉ_week,  z={Z:.3f}, L={LEAD_TIME_WEEKS}
- CS = (Œº_week * T)/2,        T={REVIEW_CYCLE_WEEKS}
- Factory buffer = (A+B total weekly mean) * {FACTORY_BUFFER_WEEKS} weeks
- Footprint = Volume / (Utilization * Effective Height)

Cadence self-check (short windows):
- Hours per week: A={HOURS_PER_WEEK_A}, B={HOURS_PER_WEEK_B}
- Windows: A={CLIENT_A_WINDOW_HOURS} h, B={CLIENT_B_WINDOW_HOURS} h
- Need_win = Œº_hour * h + z * œÉ_hour * sqrt(h)
- Flags:
  * PASS_BY_SS         : SS alone covers the window
  * PASS_BY_SS+CS      : SS+CS covers; depends on ordering cadence & arrival timing
  * RISK_WINDOW_SHORTFALL: Short-window gap; consider increasing SS, shortening T, or raising replenishment frequency.

Notes:
- Demand uses 52 weeks/year; production availability (49 weeks) is modeled in Task 1 capacity (not here).
- Parts dimensions from PDF; if palletization is needed, extend to pallets/cases and rack geometry in Task 3.
"""
        )

# =========================
# Plotting helpers
# =========================
def plot_footprint_vs_utilization(df_summary: pd.DataFrame, out_path: str):
    """
    Grouped bar: Required_Sq_Footage by Location for each Space_Utilization level.
    """
    # Pivot by utilization ‚Üí rows=Location, cols=Utilization
    pivot = df_summary.pivot_table(index="Location", columns="Space_Utilization",
                                   values="Required_Sq_Footage", aggfunc="first")
    pivot = pivot.loc[["Warehouse A","Warehouse B","Factory Outbound"]]

    ax = pivot.plot(kind="bar", figsize=(10, 6))
    ax.set_title("Required Footprint vs. Space Utilization")
    ax.set_xlabel("Location")
    ax.set_ylabel("Sq Ft")
    ax.legend(title="Utilization")
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()

def plot_cadence_coverage(df_cadence: pd.DataFrame, side: str, out_path: str):
    """
    Bar of coverage ratios (SS and SS+CS) by part, sorted by coverage ascending (top 20).
    side ‚àà {'A','B'}
    """
    cols = [f"{side}_SS_Coverage_Ratio", f"{side}_SSplusCS_Coverage_Ratio"]
    df = df_cadence.copy()
    df = df.sort_values(cols[0]).head(20)  # top 20 worst by SS coverage
    ax = df[cols].plot(kind="bar", figsize=(12, 6))
    ax.set_title(f"Cadence Coverage Ratios ({'1h' if side=='A' else '4h'} window) - Worst 20 Parts")
    ax.set_xlabel("Part")
    ax.set_ylabel("Coverage Ratio")
    ax.axhline(1.0)
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()

def plot_top_risk_parts(df_cadence: pd.DataFrame, side: str, out_path: str):
    """
    Horizontal bar of the 10 lowest SS coverage ratios.
    """
    col = f"{side}_SS_Coverage_Ratio"
    df = df_cadence.copy()
    df = df.sort_values(col).head(10)
    ax = df[col].plot(kind="barh", figsize=(8, 6))
    ax.set_title(f"Top 10 Risk Parts by SS Coverage ({'1h' if side=='A' else '4h'} window)")
    ax.set_xlabel("SS Coverage Ratio")
    ax.set_ylabel("Part")
    plt.tight_layout()
    plt.savefig(out_path, dpi=200)
    plt.close()

# =========================
# Main
# =========================
def main():
    os.makedirs(OUT_DIR, exist_ok=True)

    # 1) Parse product demand (annual & weekly)
    annual_mean_prod, weekly_mean_prod = parse_plus1_product_demand_safe(EXCEL_PATH)

    # Weekly CoV (from case prompt / PDF; adjust here if needed)
    weekly_cov_prod = pd.Series({"A1":0.15,"A2":0.20,"A3":0.20,"B1":0.12,"B2":0.18})

    # 2) BOM
    bom = read_bom(EXCEL_PATH)

    # 3) Parts stats (A/B separated)
    s_avg_A, s_avg_B, s_avg_total, s_std_A, s_std_B = compute_weekly_part_stats(
        bom, weekly_mean_prod, weekly_cov_prod
    )

    # 4) SS/CS & plan
    df_plan = compute_stocks(
        s_avg_A=s_avg_A, s_avg_B=s_avg_B, s_avg_total=s_avg_total,
        s_std_A=s_std_A, s_std_B=s_std_B,
        z=Z,
        lead_time_weeks=LEAD_TIME_WEEKS,
        review_cycle_weeks=REVIEW_CYCLE_WEEKS
    )

    # 5) Merge volume & export part-level plan
    specs = get_manual_specs_dimensions()
    df_plan_with_vol = df_plan.merge(specs, left_index=True, right_index=True)
    df_plan_with_vol.to_csv(os.path.join(OUT_DIR, "task2_storage_plan_by_part.csv"))

    # 6) Physical summary (utilization sensitivity)
    df_summary = physical_summary(
        df_plan_with_vol,
        space_utils=SPACE_UTIL_FACTORS,
        near_client_height=NEAR_CLIENT_HEIGHT_FT,
        factory_height=FACTORY_HEIGHT_FT,
        near_client_cost_per_sqft=NEAR_CLIENT_COST_PER_SQFT
    )
    df_summary.to_csv(os.path.join(OUT_DIR, "task2_physical_storage_summary.csv"), index=False)

    # 7) Cadence self-check (hourly / 4-hour)
    df_cadence = cadence_selfcheck(
        df_plan=df_plan,
        s_avg_A=s_avg_A, s_avg_B=s_avg_B,
        s_std_A=s_std_A, s_std_B=s_std_B,
        z=Z,
        hours_per_week_A=HOURS_PER_WEEK_A,
        hours_per_week_B=HOURS_PER_WEEK_B,
        win_A_hours=CLIENT_A_WINDOW_HOURS,
        win_B_hours=CLIENT_B_WINDOW_HOURS
    )
    df_cadence.to_csv(os.path.join(OUT_DIR, "task2_cadence_selfcheck_by_part.csv"))

    # 8) README
    write_readme(os.path.join(OUT_DIR, "README_Task2_Methodology.txt"))

    # 9) PLOTS
    plot_footprint_vs_utilization(
        df_summary,
        out_path=os.path.join(OUT_DIR, "plot_footprint_vs_utilization.png"),
    )
    plot_cadence_coverage(
        df_cadence,
        side="A",
        out_path=os.path.join(OUT_DIR, "plot_cadence_coverage_A.png"),
    )
    plot_cadence_coverage(
        df_cadence,
        side="B",
        out_path=os.path.join(OUT_DIR, "plot_cadence_coverage_B.png"),
    )
    plot_top_risk_parts(
        df_cadence,
        side="A",
        out_path=os.path.join(OUT_DIR, "plot_top10_risk_parts_A.png"),
    )
    plot_top_risk_parts(
        df_cadence,
        side="B",
        out_path=os.path.join(OUT_DIR, "plot_top10_risk_parts_B.png"),
    )

    print("‚úÖ Task 2 completed with plots.")
    print(f"üìÅ Outputs saved in: {OUT_DIR}")
    print("‚Ä¢ task2_storage_plan_by_part.csv")
    print("‚Ä¢ task2_physical_storage_summary.csv")
    print("‚Ä¢ task2_cadence_selfcheck_by_part.csv")
    print("‚Ä¢ README_Task2_Methodology.txt")
    print("‚Ä¢ plots: plot_footprint_vs_utilization.png, plot_cadence_coverage_*.png, plot_top10_risk_parts_*.png")

if __name__ == "__main__":
    try:
        main()
    except FileNotFoundError as e:
        print("FATAL: Excel not found. Check EXCEL_PATH:", e)
    except Exception as e:
        print("Unexpected error:", e)
